---
title: "Foucault project: Lemmatization"
author: "krissacrates"
date: "23 5 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

## Introduction

This guide will follow the process of creating right data for keyword analysis of
Foucault's archaeologies. I will try to be as generic as possible, though I focus
on outcomes important for my analysis.

I prefer using [lemmatization](https://en.wikipedia.org/wiki/Lemma_(morphology)), canonical forms of words, to [stemming](https://en.wikipedia.org/wiki/Stemming) in my keyword analysis for practical reasons. While _stems_ seem to be more compliant with computational lexical analysis,
lemmas are more natural for readers to understand and don't create confusions in some
core keywords, for instance

* word: madness -> stem: mad -> lemma: madness
* word: experience -> stem: experienc -> lemma: experience

## Prerequisites

For the POS lemmatization processing, I use *TreeTagger* sofware. It is important that you install the software corectly before you start: [www.cis.uni-muenchen.de](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/#Linux).

You can use following function to download all files into `~/tree-tagger` folder for easy installation:

```{r treeTaggerFunction}
# This preparation function downloads all the file necessary to install TreeTagger.
# TreeTagger needs to be installed in the shell.
# The source of the guide is: http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/
# This function assumes Linus environment. 
# Should you have any issues, please consult original guide and process accordingly.

treeTaggerPreparation <- function() {
    if(!file.exists("~/tree-tagger")){dir.create(path = "~/tree-tagger")}
    
    # Step 1: Download the tagger package for your system
    taggerPackLinuxPath <- "http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/tree-tagger-linux-3.2.1.tar.gz"
    download.file(url = taggerPackLinuxPath, destfile = "~/tree-tagger/tree-tagger-linux-3.2.1.tar.gz", method = "auto")
    
    # Step 2: Download the tagging scripts into the same directory
    taggingScripts <- "http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/tagger-scripts.tar.gz"
    download.file(url = taggingScripts, destfile = "~/tree-tagger/tagger-scripts.tar.gz", method = "auto")
    
    # Step 3: Download the installation script install-tagger.sh
    installationScript <- "http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/install-tagger.sh"
    download.file(url = installationScript, destfile = "~/tree-tagger/install-tagger.sh", method = "auto")
    
    # Step 4: Download the parameter files for English and French
    tagfile.english <-  "http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/english-par-linux-3.2-utf8.bin.gz"
        # Tagset documentation: http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/Penn-Treebank-Tagset.pdf
    tagfile.french <- "http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/french-par-linux-3.2-utf8.bin.gz"
        # Tagset documentation: http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/french-tagset.html
    download.file(url = tagfile.english, destfile = "~/tree-tagger/english-par-linux-3.2-utf8.bin.gz", method = "auto")
    download.file(url = tagfile.french, destfile = "~/tree-tagger/french-par-linux-3.2-utf8.bin.gz", method = "auto")
    
    # Step 5: Open a terminal window and run the installation script in the directory where you have downloaded the files:
    # sh install-tagger.sh
    
    # Step 6: Make a test, e.g.
    # echo 'Hello world!' | cmd/tree-tagger-english 
    # and 
    # echo 'Je suis Michel.' | cmd/tagger-chunker-french
    print("Now you are ready to install TreeTagger. Open a terminal window and run the installation script in the directory where you have downloaded the files: sh install-tagger.sh") 
}
```

Save the function into `treeTaggerPreparation.R` file, then `source` and call function `treeTaggerPreparation()` in the R console. Once successfull, open the terminal (shell). If you are not in '~$' home folder, get there with `cd ..` commands or similar. Then write following:

`~$ cd tree-tagger`  
  
`tree-tagger$ sh install-tagger.sh`
  
  
Test successful installation with following call:

`tree-tagger$ echo 'Hello world!' | cmd/tree-tagger-english`
  
Should you have any issues, you have to go back to the TreeTagger source.
  
Load additional libraries:

```{r libraries, cache=TRUE, eval=FALSE}
install.packages("koRpus", dependencies = TRUE)
library(koRpus)
# See documentation for 'koRpus' package: https://cran.r-project.org/web/packages/koRpus/vignettes/koRpus_vignette.pdf
```


## Text preparation

### Loading the file

Load a raw TXT file from the GitHub repository.

```{r readBook}
bookchoice <- c("History of Madness", "The Birth of the Clinic", "Order of Things", "Archaeology of Knowledge")
booksource <- c("1961_History-of-madness.txt", "1963_Birth-of-the-clinic.txt", "1966_Order-of-Things.txt", "1969_Archaeology-of-Knowledge.txt")
bookshort.choice <- c("HF", "NC", "MC", "AS")
## To run interactively, you can use menu() to choose from the books
# thebook <- menu(bookchoice, title = "Choose the book you want to analyze:")
 
## Otherwise we use History of madness in this guide. You can easily change for desired book if you reproduce the code yourself.
thebook <- 1
myBook <- booksource[thebook]
bookshort <- bookshort.choice[thebook]

if (is.na(myBook)){stop("No valid book choice! Start over...")}

if(!file.exists("./corpus-download")){dir.create("./corpus-download")}
githubFile <- paste0("https://raw.githubusercontent.com/krissacrates/foucault_keywordAnalysis/master/corpus/", myBook)
destination <- paste0("./corpus-download/", myBook)
download.file(githubFile, destfile = destination, method = "curl")
bookload <- readLines(destination)
head(bookload)
```
  

### Preparatory cleaning

The content of the book has been loaded into `bookload`. R interpretes paragraph as a line.
Now, we apply some cleaning process.

```{r cleaning}
cleanbook <- bookload
cleanbook <- tolower(cleanbook) # lowercase everything
cleanbook <- stringr::str_replace_all(cleanbook,"[^a-zA-Z\\s]", " ")    # remove everything that is not a letter
cleanbook <- stringr::str_replace_all(cleanbook,"[\\s]+", " ")  # shrink to just one space
## Credit: These functions are reused from Clean_String() function from http://www.mjdenny.com/Text_Processing_In_R.html
head(cleanbook)
```

## Tokenizing

Set the Treetagger path according to local. Here, the `treeTaggerPreparation()` function is assumed.

```{r setTreetaggerPath, cache=TRUE, eval=FALSE}
set.kRp.env(TT.cmd = "~/tree-tagger/cmd/tree-tagger-english", lang = "en")
```

Before we begin tokenization, we have to create a file from the cleaned text, because of the way TreeTagger works. Then, let's tokenize!

```{r tokenize}
if(!dir.exists("./corpus-clean")){dir.create("./corpus-clean")}
cleanbookPath <- paste0("./corpus-clean/clean_", myBook)
writeLines(cleanbook, con = cleanbookPath) # create output TXT file first
tagged.book <- treetag(cleanbookPath, lang = "en") # creates kRp.tagged class
# str(describe(tagged.book))    # summary statistics
book.df <- taggedText(tagged.book) # change kRp.tagged class into readable data frame
head(book.df, 100)  # See overview of the tokens and lemmas
```

## Tokenized data frame preparation

Clean original data frame and save it externally into CSV file.

```{r lemmasFullTable}
book.df <- book.df[, -7:-8] # remove unnecessary columns

# Use <token> if <lemma> is unknown
lemma.unknown <- which(book.df[, "lemma"] == "<unknown>") # identify rows of unknown lemmas
tokens.toreplace <- book.df[lemma.unknown, 1]   # create a vector of tokens in rows of unknown lemmas
book.df[lemma.unknown, "lemma"]  <- tokens.toreplace  # replace '<unknowns>' for tokens

# save data frame into output CSV for later use
if(!dir.exists("./output-files")){dir.create("./output-files")}
outputPathFull <- paste0("./output-files/", bookshort, "_lemmas_full.csv")
write.csv(book.df, file = outputPathFull)
head(book.df, 20)
```

Create table of unique lemmas with count and frequency. We will use unique combination of 
'lemma' and 'wclass' in order not to mix up the same lemmas with different classes (eg. 
nouns and verbs), because we will only use _nouns_ in later analysis.

```{r UniqueLemmas, message=FALSE}
library(dplyr)  # 'dplyr' package needed for the following operation
# Create data frame of a unique lemma+wclass combination with count and frequency
uniqueLemmas.df <- book.df %>%
    group_by(lemma, wclass) %>%
    summarise(n = n()) %>%
    ungroup() %>%
    mutate(freq = n /sum(n))
outputPathUnique <- paste0("./output-files/", bookshort, "_lemmas_unique.csv")
write.csv(uniqueLemmas.df, file = outputPathUnique)
```

Create the subset of data frame consisting only with _nouns_. Nouns are suitable for
keyword analysis. Based on my exploratory analysis, there is very little or no additional
value of other word classes when looking into top frequent keywords and keyword patterns used in the Foucault's archaeologies.

```{r nounsLemmas}
a <- which(uniqueLemmas.df$wclass == "noun")    # vector of row numbers with nouns
nounsLemmas.df <- uniqueLemmas.df[a, ]  # subset only nouns based on 'a' vector

# Additional metrics: absolute rank, relative rank
nouns.rank <- rank(nounsLemmas.df$freq, ties.method = "first") # vector of absolute rank
nouns.relativeRank <- nouns.rank / length(nounsLemmas.df$freq) # vector of relative rank
nounsLemmas.df <- cbind(nounsLemmas.df, "rank" = nouns.rank, "relativeRank" = nouns.relativeRank) # add mew metrics to the data frame
nounsLemmas.df <- nounsLemmas.df[order(nounsLemmas.df[, "relativeRank"], decreasing = TRUE), ] # order data frame decreasing according to the rank metric

## Additional minor manual cleanup
# Lemma 'S' is at high rank position which is obviously only inconsistency of tokenizing. Will be removed.
nounsLemmas.df <- nounsLemmas.df[-match("S", nounsLemmas.df$lemma), ]
row.names(nounsLemmas.df) <- 1:nrow(nounsLemmas.df) # change row names accordingly

# Output
outputPathNouns <- paste0("./output-files/", bookshort, "_lemmas_nouns.csv")
write.csv(nounsLemmas.df, file = outputPathNouns)
head(nounsLemmas.df, 20)
```

That's it! Data is ready for wordcloud, heatmaps and similar pieces of keyword analysis.
You can clean up all the variables, if you like. All data needed is in the output folder `./output-files`.

```{r cleanup, message=FALSE, eval=FALSE}
closeAllConnections()   # close connections to files if open
rm(list = setdiff(ls(), lsf.str()))     # remove all values except for functions
```

